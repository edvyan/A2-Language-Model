{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Name: Edvin Yang   \n",
        "Email address: st124277@ait.ac.th"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y38o7wOxjndg"
      },
      "source": [
        "# Task 1 Dataset Acquisition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MV-bYzp3jndh"
      },
      "source": [
        "In this task, \"crude\" catetory under Reuters dataset from the NLTK(Natural Language Toolkit) library will be used. The dataset's original source is Reuters News Agency, which provided a collection of news articles for research purposes.\n",
        "\n",
        "Source: https://www.nltk.org/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMjpbgZEjndi"
      },
      "source": [
        "# Task 2 Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsgLSmxpjndi"
      },
      "source": [
        "1) Detailed steps for preprocessing text data are described in form of comments.\n",
        "2) Model architecture and training processs are illustrated in following code snippet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 3 Web App"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Web app is created with DASH and can be found in folder app. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_9rmDsKjndi"
      },
      "source": [
        "### 1. Library Imports and Configure Device:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "syIXCr1Rjndi"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import torchtext, math\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tzOYCUT5jndj"
      },
      "outputs": [],
      "source": [
        "# Configure the device for training (GPU or CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qLYDzhqjndj"
      },
      "source": [
        "### 2. Download datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QO3poAJljndj",
        "outputId": "1d59f43d-ebdb-4723-ff20-e03a3f38b60a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import reuters\n",
        "\n",
        "# Download the 'reuters' dataset from the NLTK library.\n",
        "nltk.download('reuters')\n",
        "# Download the 'punkt' tokenizer models.It is used by NLTK for sentence tokenization for breaking text into individual sentences.\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naD4eevzjndk"
      },
      "source": [
        "### 3. Data Cleaning and Splitting:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9GawLtzOjndk"
      },
      "outputs": [],
      "source": [
        "# Define a function 'clean_text' that takes 'text' as input and performs text cleaning operations.\n",
        "import re\n",
        "def clean_text(text):\n",
        "    text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
        "    text = re.sub(' +', ' ', text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PTH_dF1fjndk"
      },
      "outputs": [],
      "source": [
        "# Get a list of document IDs from the Reuters corpus.\n",
        "documents = reuters.fileids()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZdjEmXq1jndk"
      },
      "outputs": [],
      "source": [
        "# Select documents that belong to the 'crude' category and store them in 'crude_docs'.\n",
        "crude_docs = [doc for doc in documents if 'crude' in reuters.categories(doc)]\n",
        "\n",
        "# Clean and concatenate the raw text from selected 'crude' category documents into a single 'text' variable.\n",
        "text = ' '.join([clean_text(reuters.raw(doc)) for doc in crude_docs])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "euGpKITXjndl"
      },
      "outputs": [],
      "source": [
        "# Tokenize the concatenated 'text' into sentences using NLTK's sentence tokenizer.\n",
        "sentences = nltk.sent_tokenize(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dr4WFGYjndl",
        "outputId": "f038dbe9-eceb-488f-8873-1522fd81893d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training set: 3276\n",
            "Number of validation set: 702\n",
            "Number of test set: 702\n"
          ]
        }
      ],
      "source": [
        "random_seed = 42\n",
        "\n",
        "# Split the 'sentences' dataset into training (70%), testing (15%), and validation (15%) sets.\n",
        "train_data, temp_data = train_test_split(sentences, test_size=0.3, random_state=random_seed)\n",
        "valid_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=random_seed)\n",
        "\n",
        "\n",
        "print(f\"Number of training set: {len(train_data)}\")\n",
        "print(f\"Number of validation set: {len(valid_data )}\")\n",
        "print(f\"Number of test set: {len(test_data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rl9NZ6Xyjndl"
      },
      "source": [
        "### 4. Data Tokenization and Vocabulary Building:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "UUR8b9K4jndl"
      },
      "outputs": [],
      "source": [
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XPHLjG4cjndl"
      },
      "outputs": [],
      "source": [
        "# Create a tokenizer for basic English text.\n",
        "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
        "\n",
        "# Define a function 'tokenize_data' that takes an 'example' and a 'tokenizer' as input.\n",
        "tokenize_data = lambda example, tokenizer: {'tokens': tokenizer(example)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "kXkQij9Xjndl"
      },
      "outputs": [],
      "source": [
        "# Tokenize the training, testing, and validation datasets using the tokenizer.\n",
        "tokenized_train_data = [{'tokens': tokenizer(example)} for example in train_data]\n",
        "tokenized_test_data = [{'tokens': tokenizer(example)} for example in test_data]\n",
        "tokenized_validation_data = [{'tokens': tokenizer(example)} for example in valid_data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "s2DePbGMjndl"
      },
      "outputs": [],
      "source": [
        "# Combine tokenized datasets into suitable lists for building the vocabulary.\n",
        "tokenized_train_dataset = [entry['tokens'] for entry in tokenized_train_data]\n",
        "tokenized_test_dataset = [entry['tokens'] for entry in tokenized_test_data]\n",
        "tokenized_validation_dataset = [entry['tokens'] for entry in tokenized_validation_data]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RI42Ixpjndm"
      },
      "source": [
        "We will tell torchtext to add any word that has occurred at least three times in the dataset to the vocabulary because otherwise it would be too big. Also we shall make sure to add unk and eos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFy4rAv1jndm",
        "outputId": "02acf6ae-a2d7-4586-879a-36afc4f72f15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7404\n"
          ]
        }
      ],
      "source": [
        "# Build a vocabulary from the tokenized training dataset.\n",
        "vocab = torchtext.vocab.build_vocab_from_iterator(tokenized_train_dataset)\n",
        "vocab.insert_token('<unk>', 0)\n",
        "vocab.insert_token('<eos>', 1)  # '<eos>' is used to indicate the end of a sequence.\n",
        "vocab.set_default_index(vocab['<unk>'])  # Set the default index for out-of-vocabulary words to '<unk>'.\n",
        "\n",
        "print(len(vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pydu09Wcjndm",
        "outputId": "a68b2bbb-c038-4d29-e1a1-b6a9bad3cb5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['<unk>', '<eos>', '.', 'the', ',', 'to', 'of', 'in', 'said', 'and']\n"
          ]
        }
      ],
      "source": [
        "print(vocab.get_itos()[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ApKKQO7mjndm"
      },
      "outputs": [],
      "source": [
        "# Save the vocabulary to 'vocab.pt'.\n",
        "torch.save(vocab, 'app/vocab.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFlx42oDjndm"
      },
      "source": [
        "### 5. Data Batching Function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ehWw1qhZjndm"
      },
      "outputs": [],
      "source": [
        "# Define a function 'get_data'\n",
        "def get_data(dataset, vocab, batch_size):\n",
        "    data = []\n",
        "    for example in dataset:\n",
        "        if example:\n",
        "            tokens = example.append('<eos>') # Add '<eos>' to the end of each sentence.\n",
        "            tokens = [vocab[token] for token in example]  # Convert each word to its corresponding numerical index.\n",
        "            data.extend(tokens)\n",
        "\n",
        "    # Convert the 'data' list to a PyTorch LongTensor.\n",
        "    data = torch.LongTensor(data)\n",
        "\n",
        "    # Calculate the number of batches based on the desired 'batch_size'.\n",
        "    num_batches = data.shape[0] // batch_size\n",
        "    data = data[:num_batches * batch_size]\n",
        "    data = data.view(batch_size, num_batches)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "XkNBgwBhjndm"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "train_data = get_data(tokenized_train_dataset, vocab, batch_size)\n",
        "valid_data = get_data(tokenized_validation_dataset, vocab, batch_size)\n",
        "test_data  = get_data(tokenized_test_dataset,  vocab, batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWzA6lGMjndm"
      },
      "source": [
        "### 6. Model Definition:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "oPhshjfjjndm"
      },
      "outputs": [],
      "source": [
        "class LSTMLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers, dropout_rate):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hid_dim    = hid_dim\n",
        "        self.emb_dim    = emb_dim\n",
        "\n",
        "        self.embedding  = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.lstm       = nn.LSTM(emb_dim, hid_dim, num_layers=num_layers, dropout=dropout_rate, batch_first=True)\n",
        "        self.dropout    = nn.Dropout(dropout_rate)\n",
        "        self.fc         = nn.Linear(hid_dim, vocab_size)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        init_range_emb = 0.1\n",
        "        init_range_other = 1/math.sqrt(self.hid_dim)\n",
        "        self.embedding.weight.data.uniform_(-init_range_emb, init_range_other)\n",
        "        self.fc.weight.data.uniform_(-init_range_other, init_range_other)\n",
        "        self.fc.bias.data.zero_()\n",
        "        for i in range(self.num_layers):\n",
        "            self.lstm.all_weights[i][0] = torch.FloatTensor(self.emb_dim,\n",
        "                self.hid_dim).uniform_(-init_range_other, init_range_other) #We\n",
        "            self.lstm.all_weights[i][1] = torch.FloatTensor(self.hid_dim,\n",
        "                self.hid_dim).uniform_(-init_range_other, init_range_other) #Wh\n",
        "\n",
        "    def init_hidden(self, batch_size, device):\n",
        "        hidden = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
        "        cell   = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
        "        return hidden, cell\n",
        "\n",
        "    def detach_hidden(self, hidden):\n",
        "        hidden, cell = hidden\n",
        "        hidden = hidden.detach() #not to be used for gradient computation\n",
        "        cell   = cell.detach()\n",
        "        return hidden, cell\n",
        "\n",
        "    def forward(self, src, hidden):\n",
        "        #src: [batch_size, seq len]\n",
        "        embedding = self.dropout(self.embedding(src)) #harry potter is\n",
        "        #embedding: [batch-size, seq len, emb dim]\n",
        "        output, hidden = self.lstm(embedding, hidden)\n",
        "        #ouput: [batch size, seq len, hid dim]\n",
        "        #hidden: [num_layers * direction, seq len, hid_dim]\n",
        "        output = self.dropout(output)\n",
        "        prediction =self.fc(output)\n",
        "        #prediction: [batch_size, seq_len, vocab_size]\n",
        "        return prediction, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5o4wgy2jndn"
      },
      "source": [
        "### 7. Model Configuration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Q2kCIx8Xjndn"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(vocab)\n",
        "emb_dim = 1024                # 400 in the paper\n",
        "hid_dim = 1024                # 1150 in the paper\n",
        "num_layers = 2                # 3 in the paper\n",
        "dropout_rate = 0.65\n",
        "lr = 1e-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hewB13lbjndn",
        "outputId": "8b360e75-26d8-42a7-89fd-7eae4d8d7c36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model has 31,964,396 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "model      = LSTMLanguageModel(vocab_size, emb_dim, hid_dim, num_layers, dropout_rate).to(device)\n",
        "optimizer  = optim.Adam(model.parameters(), lr=lr)\n",
        "criterion  = nn.CrossEntropyLoss()\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'The model has {num_params:,} trainable parameters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUloQUxujndn"
      },
      "source": [
        "### 8. Batch Preparation Function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "s9JC_hnpjndn"
      },
      "outputs": [],
      "source": [
        "def get_batch(data, seq_len, idx):\n",
        "    #data #[batch size, bunch of tokens]\n",
        "    src    = data[:, idx:idx+seq_len]\n",
        "    target = data[:, idx+1:idx+seq_len+1]  #target simply is ahead of src by 1\n",
        "    return src, target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmZ_uOn6jndn"
      },
      "source": [
        "### 9. Training Function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "QEsaA76ljndn"
      },
      "outputs": [],
      "source": [
        "def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    model.train()\n",
        "    # drop all batches that are not a multiple of seq_len\n",
        "    # data #[batch size, seq len]\n",
        "    num_batches = data.shape[-1]\n",
        "    data = data[:, :num_batches - (num_batches -1) % seq_len]  #we need to -1 because we start at 0\n",
        "    num_batches = data.shape[-1]\n",
        "\n",
        "    #reset the hidden every epoch\n",
        "    hidden = model.init_hidden(batch_size, device)\n",
        "\n",
        "    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ',leave=False):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #hidden does not need to be in the computational graph for efficiency\n",
        "        hidden = model.detach_hidden(hidden)\n",
        "\n",
        "        src, target = get_batch(data, seq_len, idx) #src, target: [batch size, seq len]\n",
        "        src, target = src.to(device), target.to(device)\n",
        "        batch_size = src.shape[0]\n",
        "        prediction, hidden = model(src, hidden)\n",
        "\n",
        "        #need to reshape because criterion expects pred to be 2d and target to be 1d\n",
        "        prediction = prediction.reshape(batch_size * seq_len, -1)  #prediction: [batch size * seq len, vocab size]\n",
        "        target = target.reshape(-1)\n",
        "        loss = criterion(prediction, target)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item() * seq_len\n",
        "    return epoch_loss / num_batches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eYtHNFJjndn"
      },
      "source": [
        "### 10. Evaluation Function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "zCYhVdAUjndn"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    model.eval()\n",
        "    num_batches = data.shape[-1]\n",
        "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
        "    num_batches = data.shape[-1]\n",
        "\n",
        "    hidden = model.init_hidden(batch_size, device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx in range(0, num_batches - 1, seq_len):\n",
        "            hidden = model.detach_hidden(hidden)\n",
        "            src, target = get_batch(data, seq_len, idx)\n",
        "            src, target = src.to(device), target.to(device)\n",
        "            batch_size= src.shape[0]\n",
        "\n",
        "            prediction, hidden = model(src, hidden)\n",
        "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
        "            target = target.reshape(-1)\n",
        "\n",
        "            loss = criterion(prediction, target)\n",
        "            epoch_loss += loss.item() * seq_len\n",
        "    return epoch_loss / num_batches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zu99M1Bjndo"
      },
      "source": [
        "### 11. Training Loop:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7_VTigXjndo"
      },
      "source": [
        "Follows very basic procedure. One note is that some of the sequences that will be fed to the model may involve parts from different sequences in the original dataset or be a subset of one (depending on the decoding length). For this reason we will reset the hidden state every epoch, this is like assuming that the next batch of sequences is probably always a follow up on the previous in the original dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVcFlDiMjndo",
        "outputId": "ec1c46c0-6e9c-4561-e561-881745cd46e8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Perplexity: 830.553\n",
            "\tValid Perplexity: 603.404\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Perplexity: 503.127\n",
            "\tValid Perplexity: 398.726\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Perplexity: 349.864\n",
            "\tValid Perplexity: 318.659\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Perplexity: 280.327\n",
            "\tValid Perplexity: 267.742\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Perplexity: 229.223\n",
            "\tValid Perplexity: 235.110\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Perplexity: 193.302\n",
            "\tValid Perplexity: 210.282\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Perplexity: 166.559\n",
            "\tValid Perplexity: 194.332\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Perplexity: 144.612\n",
            "\tValid Perplexity: 182.184\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Perplexity: 127.507\n",
            "\tValid Perplexity: 172.989\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Perplexity: 112.697\n",
            "\tValid Perplexity: 166.259\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Perplexity: 100.794\n",
            "\tValid Perplexity: 163.566\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Perplexity: 89.998\n",
            "\tValid Perplexity: 157.723\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Perplexity: 81.246\n",
            "\tValid Perplexity: 155.146\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Perplexity: 73.027\n",
            "\tValid Perplexity: 154.237\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Perplexity: 66.369\n",
            "\tValid Perplexity: 151.237\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Perplexity: 59.948\n",
            "\tValid Perplexity: 149.899\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Perplexity: 54.639\n",
            "\tValid Perplexity: 149.659\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Perplexity: 50.031\n",
            "\tValid Perplexity: 150.222\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Perplexity: 44.039\n",
            "\tValid Perplexity: 150.681\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Perplexity: 39.974\n",
            "\tValid Perplexity: 147.991\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Perplexity: 38.180\n",
            "\tValid Perplexity: 147.373\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Perplexity: 36.944\n",
            "\tValid Perplexity: 148.207\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Perplexity: 35.407\n",
            "\tValid Perplexity: 147.996\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Perplexity: 34.586\n",
            "\tValid Perplexity: 147.798\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Perplexity: 33.925\n",
            "\tValid Perplexity: 148.174\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Perplexity: 33.901\n",
            "\tValid Perplexity: 148.295\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Perplexity: 33.790\n",
            "\tValid Perplexity: 148.318\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Perplexity: 33.694\n",
            "\tValid Perplexity: 148.326\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Perplexity: 33.795\n",
            "\tValid Perplexity: 148.348\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Perplexity: 33.708\n",
            "\tValid Perplexity: 148.347\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Perplexity: 33.756\n",
            "\tValid Perplexity: 148.353\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Perplexity: 33.640\n",
            "\tValid Perplexity: 148.357\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Perplexity: 33.696\n",
            "\tValid Perplexity: 148.358\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Perplexity: 33.730\n",
            "\tValid Perplexity: 148.358\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Perplexity: 33.719\n",
            "\tValid Perplexity: 148.359\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Perplexity: 33.627\n",
            "\tValid Perplexity: 148.359\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Perplexity: 33.746\n",
            "\tValid Perplexity: 148.359\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Perplexity: 33.677\n",
            "\tValid Perplexity: 148.359\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Perplexity: 33.516\n",
            "\tValid Perplexity: 148.359\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Perplexity: 33.727\n",
            "\tValid Perplexity: 148.359\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Perplexity: 33.538\n",
            "\tValid Perplexity: 148.359\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Perplexity: 33.682\n",
            "\tValid Perplexity: 148.359\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Perplexity: 33.709\n",
            "\tValid Perplexity: 148.359\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Perplexity: 33.658\n",
            "\tValid Perplexity: 148.359\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Perplexity: 33.533\n",
            "\tValid Perplexity: 148.360\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Perplexity: 33.679\n",
            "\tValid Perplexity: 148.360\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Perplexity: 33.710\n",
            "\tValid Perplexity: 148.360\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Perplexity: 33.680\n",
            "\tValid Perplexity: 148.360\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Perplexity: 33.713\n",
            "\tValid Perplexity: 148.360\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Perplexity: 33.516\n",
            "\tValid Perplexity: 148.360\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 50\n",
        "seq_len  = 50 #<----decoding length\n",
        "clip    = 0.25\n",
        "\n",
        "# Initializes a learning rate scheduler.\n",
        "# The learning rate scheduler reduces the learning rate (lr) during training if the validation loss stops improving.\n",
        "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "# Enters a loop that iterates over the specified number of epochs\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = train(model, train_data, optimizer, criterion,\n",
        "                batch_size, seq_len, clip, device)\n",
        "    valid_loss = evaluate(model, valid_data, criterion, batch_size,\n",
        "                seq_len, device)\n",
        "\n",
        "    lr_scheduler.step(valid_loss)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'app/best-val-lstm_lm.pt')\n",
        "\n",
        "    print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n",
        "    print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcexifPfjndp"
      },
      "source": [
        "### 12. Testing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MSJeeC9jndp",
        "outputId": "b30228b7-c4e6-48ed-bd44-e25e319d02de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Perplexity: 166.933\n"
          ]
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('app/best-val-lstm_lm.pt',  map_location=device))\n",
        "test_loss = evaluate(model, test_data, criterion, batch_size, seq_len, device)\n",
        "print(f'Test Perplexity: {math.exp(test_loss):.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZ2lTq2Ijndp"
      },
      "source": [
        "### 13. Real-world inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPEOE4fhjndp"
      },
      "source": [
        "Here we take the prompt, tokenize, encode and feed it into the model to get the predictions. We then apply softmax while specifying that we want the output due to the last word in the sequence which represents the prediction for the next word. We divide the logits by a temperature value to alter the model’s confidence by adjusting the softmax probability distribution.\n",
        "\n",
        "Once we have the Softmax distribution, we randomly sample it to make our prediction on the next word. If we get then we give that another try. Once we get we stop predicting.\n",
        "\n",
        "We decode the prediction back to strings last lines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "sRTmFprBjndp"
      },
      "outputs": [],
      "source": [
        "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed=None):\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "    model.eval()\n",
        "    tokens = tokenizer(prompt)\n",
        "    indices = [vocab[t] for t in tokens]\n",
        "    batch_size = 1\n",
        "    hidden = model.init_hidden(batch_size, device)\n",
        "    with torch.no_grad():\n",
        "        for i in range(max_seq_len):\n",
        "            src = torch.LongTensor([indices]).to(device)\n",
        "            prediction, hidden = model(src, hidden)\n",
        "\n",
        "            #prediction: [batch size, seq len, vocab size]\n",
        "            #prediction[:, -1]: [batch size, vocab size] #probability of last vocab\n",
        "\n",
        "            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)\n",
        "            prediction = torch.multinomial(probs, num_samples=1).item()\n",
        "\n",
        "            while prediction == vocab['<unk>']: #if it is unk, we sample again\n",
        "                prediction = torch.multinomial(probs, num_samples=1).item()\n",
        "\n",
        "            if prediction == vocab['<eos>']:    #if it is eos, we stop\n",
        "                break\n",
        "\n",
        "            indices.append(prediction) #autoregressive, thus output becomes input\n",
        "\n",
        "    itos = vocab.get_itos()\n",
        "    tokens = [itos[i] for i in indices]\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IurN1LE_jndq",
        "outputId": "23a7b9bb-a9e8-4aab-a2d8-cb1bf5e9e6c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.5\n",
            "petrol gas is the first oil company , he said .\n",
            "\n",
            "0.7\n",
            "petrol gas is the four countries , including reuters .\n",
            "\n",
            "0.75\n",
            "petrol gas is half by much in the market .\n",
            "\n",
            "0.8\n",
            "petrol gas is half by much in the market .\n",
            "\n",
            "1.0\n",
            "petrol gas is half by much yesterday , al-wattari said .\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt = 'Petrol gas is '\n",
        "max_seq_len = 30\n",
        "seed = 0\n",
        "\n",
        "#smaller the temperature, more diverse tokens but comes\n",
        "#with a tradeoff of less-make-sense sentence\n",
        "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\n",
        "for temperature in temperatures:\n",
        "    generation = generate(prompt, max_seq_len, temperature, model, tokenizer,\n",
        "                          vocab, device, seed)\n",
        "    print(str(temperature)+'\\n'+' '.join(generation)+'\\n')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
